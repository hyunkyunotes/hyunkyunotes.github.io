---
title: CS 224N Notes
categories: [ML]
tags: [CS224N]
datacamp: 1
maths: 1
toc: 1
date: 2025-04-29
---

{% include toc.html %}

## Co-occurence

- Word2Vec
	- center word, context word within certain window, minimize cost function -> minimize exponential
- GloVe
	- Similarity based on ratio of probability of two words' occurrence.
- Cosine Similarity
	- `similarity` $=\\cos\\theta =\\frac{p\\cdot q}{\Vert p\Vert\Vert q\Vert}$
	- Cosine Distance = `1 - similarity`
	- `man : grandfather; woman : x`: $x=(g-m)+w$
		- does not always work due to factors like polysemy (multiple meanings), frequency, etc.
		- e.g. `hand : glove :: foot : socks` does not work due to meanings of sock, items like shoes, etc.
		- Moreover, bias in historical contexts where the dictionary is developed from may introduce bias like gender

## Network Architecture

### Biological Motivation

- 'Neurons' connected by 'synapses', receives signal from 'dendrites' outputs through 'axons'
- Interact multiplicatively ($w_0x_0$)
- Firing rate represented by activation function $f$

![](/assets/img/neuron.png){: .no-border}

#### Simple Neural Network

- $s=Wx$ Assuming $W[10 x 3072], x[3072, 1]$, outputs vector of size 10.
- introduce non-linearity element wise
	- e.g. $s=W_2\max(0, W_1x)$, aim is to obtain size 10 vector again. Can recursively continue

#### Single neuron as classifier

- neuron can 'like' (activation near 1), or 'dislike' (near) in certain linear regions

{% highlight python linenos %}
class Neuron(object):
  # ... 
  def forward(self, inputs):
    """ assume inputs and weights are 1-D numpy arrays and bias is a number """
    cell_body_sum = np.sum(inputs * self.weights) + self.bias
    firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid activation function
    return firing_rate
{% endhighlight %}

### Neuron types

#### Sigmoid Drawbacks

- Saturate and kill gradients (near tails), leading to no recursive signal flow (updating is minimal)
- Always positive -> gradient will become all positive or all negative

#### Tanh

- Preferred over sigmoid due to zero-centered, but still kills gradient
- $\tanh(x)=2\sigma(2x)-1$

#### ReLU

- Rectified linear unit
- Computes $f(x)=\max(0,x)$ -> effectively eliminate all $x<0$. 
- Accelerates convergence of gradient descent due to linearity
- cheap cost in implementation
- BUT, can die during training (large gradient can cause a big update/change and neuron may never activate again)


#### Leaky ReLU

- Attempt to fix dying ReLU
- Small slope in negative region instead of 0

### Figure/Architecture

- 'Fully-connected-layer': no connections within single layer, each neuron in a layer will connect to all neurons in next

![](/assets/img/sample-layer.png){: .no-border}

#### Naming Conventions

- Don't count input layer (i.e. 1-layer corresponds to no hidden layer)
- Neural Network (NN) = Artificial Neural Network (ANN) = Multi-Layer Perceptron (MLP)

#### Output Layer

- Does not have activation function
- usually taken to represent some real-valued target

#### Sizing Neural Networks

- Don't count input layer
	- e.g. in left layer, 6 neurons + $[3\times 4]+[4\times 2]$ = 26 learnable parameters
	- Number of neurons + number of weights


#### Example feed-forward computation (2nd network)

{% highlight python linenos %}
# forward-pass of a 3-layer neural network:
f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)
x = np.random.randn(3, 1) # random input vector of three numbers (3x1)
h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations (4x1)
h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activations (4x1)
out = np.dot(W3, h2) + b3 # output neuron (1x1)
{% endhighlight %}

- $W_1,W_2,W_3, b_1,b_2,b_3$ learnable parameters

#### Representational Power

- Theoretically, NN with one hidden layer is a universal approximation
- However, in practice, 2-layer works better (efficient) than 1, 3 than 2, but change decreases for 4 than 3

#### Selecting N-layers and size

![](/assets/img/fitting.png){: .no-border}

- Should avoid overfitting (20 neuron case)
- Assuming outliers -> lead to generalization

## Back-Propagation

### Notation

- $f(x,y)=\max(x,y)\implies \partial f_x = \mathbb{1}(x\geq y)$

- Computing gradients through recursive chain rule

<div class='mt-2 mb-2' data-datacamp-exercise data-lang='python'>
<code data-type='sample-code'>
import math 

w = [2,-3,-3] # assume some random weights and data
x = [-1, -2]

# forward pass
dot = w[0]*x[0] + w[1]*x[1] + w[2]
f = 1.0 / (1 + math.exp(-dot)) # sigmoid function

# backward pass through the neuron (backpropagation)
ddot = (1 - f) * f # gradient on dot variable, using the sigmoid gradient derivation
dx = [w[0] * ddot, w[1] * ddot] # backprop into x
dw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # backprop into w
# we're done! we have the gradients on the inputs to the circuit
</code>
</div>